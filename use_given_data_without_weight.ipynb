{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the original data\n",
    "import pickle\n",
    "with open('./train_data', 'rb') as f:\n",
    "    train_data = pickle.load(f)\n",
    "    train_label= pickle.load(f)\n",
    "with open('./test_data', 'rb') as f:\n",
    "    test_data = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random as rn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50000, 32, 32, 3) (10000, 32, 32, 3) (50000, 1)\n"
     ]
    }
   ],
   "source": [
    "#transform the data to correct image form\n",
    "\n",
    "#train data\n",
    "a=[]\n",
    "for i in range(50000):\n",
    "    single_img = np.array(train_data[i])\n",
    "    single_img_reshaped = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "    a.append(single_img_reshaped)\n",
    "train_data=np.array(a)\n",
    "\n",
    "#train label\n",
    "train_label=np.array(train_label).reshape(50000,1)\n",
    "\n",
    "#test data\n",
    "b=[]\n",
    "for i in range(10000):\n",
    "    single_img = np.array(test_data[i])\n",
    "    single_img_reshaped = np.transpose(np.reshape(single_img,(3, 32,32)), (1,2,0))\n",
    "    b.append(single_img_reshaped)\n",
    "test_data=np.array(b)\n",
    "\n",
    "print(train_data.shape,test_data.shape,train_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "t=train_data/255.0\n",
    "\n",
    "# show the data through plot\n",
    "plt.figure()  # create new figure\n",
    "fig_size = [20, 20]  # specify figure size\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size  # set figure size\n",
    "\n",
    "# Plot first 100 train image of dataset\n",
    "for i in range(1, 101):\n",
    "    ax = plt.subplot(10, 10, i)  # Specify the i'th subplot of a 10*10 grid\n",
    "    img = t[i, :, :, :]  # Choose i'th image from train data\n",
    "    ax.get_xaxis().set_visible(False)  # Disable plot axis.\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    plt.imshow(img)\n",
    "\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The below is necessary in Python 3.2.3 onwards to\n",
    "# have reproducible behavior for certain hash-based operations.\n",
    "\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = '0'\n",
    "\n",
    "# The below is necessary for starting Numpy generated random numbers\n",
    "# in a well-defined initial state.\n",
    "\n",
    "np.random.seed(8)\n",
    "\n",
    "# The below is necessary for starting core Python generated random numbers\n",
    "# in a well-defined state.\n",
    "\n",
    "rn.seed(80)\n",
    "\n",
    "# Force TensorFlow to use single thread.\n",
    "# Multiple threads are a potential source of\n",
    "# non-reproducible results.\n",
    "\n",
    "session_conf = tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "\n",
    "# The below tf.set_random_seed() will make random number generation\n",
    "# in the TensorFlow backend have a well-defined initial state.\n",
    "# For further details, see: https://www.tensorflow.org/api_docs/python/tf/set_random_seed\n",
    "\n",
    "tf.set_random_seed(800)\n",
    "\n",
    "sess = tf.Session(graph=tf.get_default_graph(), config=session_conf)\n",
    "K.set_session(sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Flatten, Dense, Activation\n",
    "from keras import optimizers\n",
    "from keras import regularizers\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import np_utils\n",
    "import math\n",
    "from scipy.stats import binom\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16_CIFAR100:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.num_classes = 100\n",
    "        self.weight_decay = 0.0005\n",
    "        self.x_shape = [32,32,3]\n",
    "        self.batch_size = 128\n",
    "        self.epoches = 100\n",
    "        self.learning_rate = 0.1\n",
    "        self.lr_decay = 1e-6\n",
    "        \n",
    "    # Function to create dataset for training and validation of model\n",
    "    def create_dataset(self): \n",
    "        \n",
    "        num_classes = self.num_classes\n",
    "        \n",
    "        # Create Train and Test datasets:\n",
    "        train_dat=train_data.reshape((train_data.shape[0],3072)) #reshape to(50000,3072)\n",
    "        \n",
    "        train_dat_df=pd.DataFrame(train_dat) #transform to dataframe\n",
    "        train_label_df=pd.DataFrame(train_label)\n",
    "        \n",
    "        x_train_df=train_dat_df.sample(frac=0.9,axis=0) #use 90% in training set as x_train\n",
    "        y_train_df=train_label_df.iloc[list(x_train_df.index)]\n",
    "        \n",
    "        bad_df = train_dat_df.index.isin(list(x_train_df.index))\n",
    "        x_test_df=train_dat_df[~bad_df] # remaining 10% as x_test\n",
    "        y_test_df=train_label_df.iloc[list(x_test_df.index)]\n",
    "        \n",
    "        x_train=np.array(x_train_df).reshape((np.array(x_train_df).shape[0],32,32,3)).astype('float32')\n",
    "        x_test =np.array(x_test_df).reshape((np.array(x_test_df).shape[0],32,32,3)).astype('float32')\n",
    "        y_train=np.array(y_train_df).reshape((np.array(x_train_df).shape[0],1))\n",
    "        y_test=np.array(y_test_df).reshape((np.array(x_test_df).shape[0],1))\n",
    "        \n",
    "        x_test_p=test_data.astype('float32') #test set that needs to be predicted\n",
    "        \n",
    "        # Normalize the data\n",
    "        x_train, x_test, x_test_p= self.normalize(x_train, x_test, x_test_p)\n",
    "        \n",
    "        # Create one-hot encodings\n",
    "        y_train = np_utils.to_categorical(y_train, num_classes)\n",
    "        y_test = np_utils.to_categorical(y_test, num_classes)\n",
    "        \n",
    "        return x_train, y_train, x_test, y_test, x_test_p\n",
    "    \n",
    "    # Function to normalize train and validation datasets\n",
    "    def normalize(self,X_train, X_test, X_test_p): \n",
    "        \n",
    "        # Compute Mean\n",
    "        mean = np.mean(X_train,axis=(0, 1, 2, 3))\n",
    "        \n",
    "        # Compute Standard Deviation\n",
    "        std = np.std(X_train, axis=(0, 1, 2, 3)) \n",
    "        \n",
    "        # Normalize the data\n",
    "        X_train = (X_train-mean)/(std+1e-7)\n",
    "        X_test = (X_test-mean)/(std+1e-7)\n",
    "        X_test_p = (X_test_p-mean)/(std+1e-7)\n",
    "        \n",
    "        return X_train, X_test, X_test_p\n",
    "    \n",
    "    def buildmodel(self): \n",
    "        \n",
    "        weight_decay = self.weight_decay\n",
    "        num_classes = self.num_classes\n",
    "        x_shape = self.x_shape\n",
    "        \n",
    "        model = Sequential()\n",
    "        \n",
    "        # First group of convolutional layer\n",
    "        model.add(Conv2D(64, (3, 3), padding='same', input_shape = x_shape,kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.3))\n",
    "\n",
    "        model.add(Conv2D(64, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Second group of convolutional layer\n",
    "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "        \n",
    "\n",
    "        model.add(Conv2D(128, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Third group of convolutional layer\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(256, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Fourth group of convolutional layer\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Fifth group of convolutional layer\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.4))\n",
    "\n",
    "        model.add(Conv2D(512, (3, 3), padding='same',kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "        \n",
    "        # Two Fully connected layer\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(512, kernel_regularizer=regularizers.l2(weight_decay)))\n",
    "        model.add(Activation('relu'))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.5))\n",
    "        \n",
    "        model.add(Dense(num_classes))\n",
    "        model.add(Activation('softmax'))\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def model_train(self, model, x_train, y_train, x_test, y_test):\n",
    "        \n",
    "        # Training parameters\n",
    "        batch_size = self.batch_size\n",
    "        number_epoches = self.epoches\n",
    "        learning_rate = self.learning_rate\n",
    "        lr_decay = self.lr_decay\n",
    "\n",
    "            # Data augmentation\n",
    "        dataaugmentation = ImageDataGenerator(\n",
    "                                featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "                                samplewise_center=False,  # set each sample mean to 0\n",
    "                                featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "                                samplewise_std_normalization=False,  # divide each input by its std\n",
    "                                zca_whitening=False,  # apply ZCA whitening\n",
    "                                rotation_range=15,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "                                width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "                                height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "                                horizontal_flip=True,  # randomly flip images\n",
    "                                vertical_flip=False)  # randomly flip images\n",
    "        \n",
    "        dataaugmentation.fit(x_train)\n",
    "            \n",
    "        # Optimization details\n",
    "        sgd = optimizers.SGD(lr=0.0, decay=lr_decay, momentum=0.9, nesterov=True)\n",
    "        model.compile(loss='categorical_crossentropy', optimizer=sgd,metrics=['accuracy'])\n",
    "\n",
    "\n",
    "        # Function to reduce learning rate by half after every 25 epochs\n",
    "        def step_decay(epoch):\n",
    "            # LearningRate = InitialLearningRate * DropRate^floor(Epoch / EpochDrop)\n",
    "        \n",
    "            initial_lrate = 0.1\n",
    "            drop = 0.5\n",
    "            epochs_drop = 25.0\n",
    "            lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "            return lrate\n",
    "\n",
    "        # Callback for learning rate schedule\n",
    "        lrate = LearningRateScheduler(step_decay)\n",
    "        callbacks_list = [lrate]\n",
    "\n",
    "        # spe = Steps per epoch\n",
    "        spe = x_train.shape[0] // batch_size\n",
    "        \n",
    "        # Fit the model\n",
    "        model.fit_generator(dataaugmentation.flow(x_train, y_train,\n",
    "                                                  batch_size = batch_size),\n",
    "                            steps_per_epoch = spe, callbacks=callbacks_list,\n",
    "                            epochs = number_epoches,\n",
    "                            validation_data = (x_test, y_test))\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "351/351 [==============================] - 2483s 7s/step - loss: 22.4789 - acc: 0.0279 - val_loss: 23.9973 - val_acc: 0.0084\n",
      "Epoch 2/100\n",
      "351/351 [==============================] - 2433s 7s/step - loss: 13.7472 - acc: 0.0422 - val_loss: 11.3182 - val_acc: 0.0150\n",
      "Epoch 3/100\n",
      "351/351 [==============================] - 2442s 7s/step - loss: 9.3732 - acc: 0.0502 - val_loss: 8.1251 - val_acc: 0.0318\n",
      "Epoch 4/100\n",
      "351/351 [==============================] - 2432s 7s/step - loss: 7.0046 - acc: 0.0559 - val_loss: 8.6841 - val_acc: 0.0204\n",
      "Epoch 5/100\n",
      "351/351 [==============================] - 2434s 7s/step - loss: 6.5605 - acc: 0.0350 - val_loss: 6.3180 - val_acc: 0.0226\n",
      "Epoch 6/100\n",
      "351/351 [==============================] - 2437s 7s/step - loss: 5.6709 - acc: 0.0495 - val_loss: 11.6307 - val_acc: 0.0134\n",
      "Epoch 7/100\n",
      "351/351 [==============================] - 2437s 7s/step - loss: 4.8399 - acc: 0.0686 - val_loss: 11.1310 - val_acc: 0.0194\n",
      "Epoch 8/100\n",
      "351/351 [==============================] - 2435s 7s/step - loss: 4.3976 - acc: 0.0890 - val_loss: 5.9443 - val_acc: 0.0308\n",
      "Epoch 9/100\n",
      "351/351 [==============================] - 2439s 7s/step - loss: 4.1713 - acc: 0.1031 - val_loss: 4.8124 - val_acc: 0.0636\n",
      "Epoch 10/100\n",
      "351/351 [==============================] - 2438s 7s/step - loss: 4.0358 - acc: 0.1193 - val_loss: 4.3090 - val_acc: 0.0988\n",
      "Epoch 11/100\n",
      "351/351 [==============================] - 2439s 7s/step - loss: 3.9461 - acc: 0.1304 - val_loss: 4.0570 - val_acc: 0.1478\n",
      "Epoch 12/100\n",
      "351/351 [==============================] - 2441s 7s/step - loss: 3.8597 - acc: 0.1498 - val_loss: 3.9691 - val_acc: 0.1400\n",
      "Epoch 13/100\n",
      "351/351 [==============================] - 2441s 7s/step - loss: 3.8048 - acc: 0.1705 - val_loss: 3.8385 - val_acc: 0.1732\n",
      "Epoch 14/100\n",
      "351/351 [==============================] - 2538s 7s/step - loss: 3.7602 - acc: 0.1928 - val_loss: 3.8103 - val_acc: 0.1902\n",
      "Epoch 15/100\n",
      "173/351 [=============>................] - ETA: 21:33 - loss: 3.7458 - acc: 0.2058"
     ]
    }
   ],
   "source": [
    "# Create class object\n",
    "model_cifar100 = VGG16_CIFAR100()\n",
    "\n",
    "# Training and validation datasets\n",
    "x_train, y_train, x_test, y_test, x_test_p = model_cifar100.create_dataset()\n",
    "\n",
    "# Create model\n",
    "model = model_cifar100.buildmodel()\n",
    "\n",
    "# Train the model\n",
    "model = model_cifar100.model_train(model, x_train, y_train, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction on test set\n",
    "predict_test = model.predict(x_test_p)\n",
    "pred=np.argmax(predict_test,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(pred)\n",
    "df.index.name='ids'\n",
    "df.columns=['labels']\n",
    "df.to_csv('final_result.csv',header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
